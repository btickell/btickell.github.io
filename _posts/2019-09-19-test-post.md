---
layout: post
title: Maximum Likelihood Estimation (MLE)
tags: [MLE, maximum likelihood estimation, stats, python, lecture]
categories: [math, statistics, Stat Learning Series]
date: 2019-09-19 16:55 -0700
---

## Introduction
As data scientists we often have a set of observed data and want to "describe" the underlying distribution from which it was generated. The act of describing this distribution is known as [**fitting**](https://en.wikipedia.org/wiki/Probability_distribution_fitting) the distribution to the data. 

**Maximum likelihood estimation** (MLE) is one such method of distribution fitting in which we consider the likelihood of our data under the proposed distribution and tune the distribution parameters to maximize this likelihood. In this way we find the parameters that yield the highest probability of generating our given sample restricted to the given form of distribution. Let's make this more concrete.

## The Likelihood Function

What do we mean by the *likelihood* of our data? Well, first assume that our data is a single sample $$X_1$$ comes from the probability distribution described by the probability distribution function $$f(x)$$. Then the probability that we sample $$X_1 = x$$ is simply $$f(x)$$. Now assume we have a series of $$n$$ independent samples $$X_1, \dots, X_n$$ from the same distribution. Then, by independence, the probability of this sample is,

$$P(X_1=x_1, ..., X_n=x_n) = f(x_1)*\dots*f(x_n)$$

Suppose that our distribution has a single parameter $$\theta$$. Then the probability above will obviously be different for different values of $$\theta$$. What is our best choice of $$\theta$$ when estimating the distribution? Consider the following *likelihood function*:

$$\begin{align}
\mathcal{L}(\theta \mid X_1, \dots X_n) &= P(X_1=x_1, \dots X_n=x_n \mid \theta) \\
&= \prod_{i=1}^n P(X_i=x_i \mid \theta) \\
\end{align}$$

This function describes how likely our data is for a given value of $$\theta$$. A natural estimate of $$\theta$$ can be chosen to maximize this likelihood.

## Example 1: Poisson Distribution
The poisson distribution has probability distribution function:

$$P(X=k) = \frac{\lambda^k e^{-\lambda}}{k!}$$

Then the likelihood function is,

$$ \mathcal{L}(\lambda \mid X_1, \dots X_n) = \prod_{i=1}^n \frac{\lambda^{x_i} e^{-\lambda}}{x_i!} $$

How do we maximize the value? It is often easier to maximize the *log-likelihood* and this will yield the same result as the log is monotonically increasing.

The log likelihood in this case is:

$$\begin{align}
l(\lambda \mid X_1, \dots , X_n) &= \log{\prod_{i=1}^n \frac{\lambda^{x_i} e^{-\lambda}}{x_i!}} \\
&= \sum_{i=1}^n \log{\lambda^{x_i}e^{-\lambda}} - \sum_{i=1}^n \log{x_i!}\\
&= \sum_{i=1}^n x_i\log{\lambda} - \sum_{i=1}^n \lambda - \sum_{i=1}^n \log{x_i!}\\
\end{align}$$

To maximize this we set the derivative w.r.t $$\lambda$$ to zero.

$$\frac{dl}{d\lambda} = \frac{1}{\lambda} \sum_{i=1}^n x_i - n = 0$$

Solving this yields our estimate $$\hat{\lambda} = \frac{1}{n} \sum_{i=1}^n x_i$$

This is the sample mean of our data and matches our intuition as the theoretical mean of the poisson distribution with parameter $$\lambda$$ is $$\lambda$$.

Let's see this in code:

```python
import numpy as np
np.random.seed(0)
lam = 15
sample = np.random.poisson(lam, size=100) # Generate a sample of 100 i.i.d poisson variables
lam_hat = np.mean(sample)
print(lam_hat)

>>> 15.22
```
This code generates a sample of 100 poisson random variables, each with $$\lambda = 15$$. The mle, `lam_hat` yields an estimate of 15.22, which is fairly close. Let's compare the distribution functions of the true distribution and our MLE distribution,

```python
import matplotlib.pyplot as plt
import math

def poiss(lam, k):
	return (np.exp(-1*lam)*(lam**k)) / np.math.factorial(k)

mle_pdf = np.zeros(50)
true_pdf = np.zeros(50)
for k in range(50):
	true_pdf[k] = poiss(lam, k)
	mle_pdf[k] = poiss(lam_hat, k)

plt.plot(range(50), true_pdf, ls='--', label='True Distribution', alpha=.8)
plt.plot(range(50), mle_pdf,  ls='--', label='MLE Distribution', alpha=.8)
plt.legend()
plt.show()
```
![image](/figures/mle-post1/mle_poiss_fig.png)

<!-- ## Example 2: Normal Distribution
The normal distribution has probability distribution function:

$$f(x) = \frac{1}{\sqrt{2\pi}} e^{-(x - \mu)^2 / \sigma^2}$$ -->
